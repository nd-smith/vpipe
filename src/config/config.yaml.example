# =============================================================================
# EventHub Pipeline Configuration
# =============================================================================
# This file configures the EventHub-based event processing pipeline for both
# Verisk and ClaimX domains. Each worker can be individually configured with
# batch sizes, time windows, and performance settings.
#
# Configuration is loaded ONLY from this YAML file.
# Environment variables are NOT supported.
#
# NOTE: Kafka has been removed - EventHub is used for all pipeline communication
# =============================================================================

# =============================================================================
# PIPELINE EVENT SOURCE
# =============================================================================
# Determines which event source the pipeline uses for ingestion
# Options:
#   - "eventhouse": Production source - Microsoft Fabric Eventhouse
#   - "eventhub": Production source - Azure Event Hub
#   - "dummy": Test source - generates realistic synthetic data locally
event_source: "eventhouse"

# =============================================================================
# PIPELINE CONFIGURATION
# =============================================================================
pipeline:
  # ===========================================================================
  # VERISK DOMAIN CONFIGURATION (formerly XACT)
  # Verisk property claim assignments and attachments pipeline
  # ===========================================================================
  verisk:
    # -------------------------------------------------------------------------
    # EventHub Entity Names (formerly Kafka topics)
    # -------------------------------------------------------------------------
    topics:
      events: "com.allstate.pcesdopodappv1.verisk.events.raw"                  # Raw events from Event Hub/Eventhouse
      enrichment_pending: "com.allstate.pcesdopodappv1.verisk.enrichment.pending"  # Events awaiting enrichment
      downloads_pending: "com.allstate.pcesdopodappv1.verisk.downloads.pending"  # Download tasks awaiting processing
      downloads_cached: "com.allstate.pcesdopodappv1.verisk.downloads.cached"    # Downloads cached locally, awaiting upload
      downloads_results: "com.allstate.pcesdopodappv1.verisk.downloads.results"  # Upload results for Delta processing
      dlq: "com.allstate.pcesdopodappv1.verisk.downloads.dlq"                    # Dead letter queue for failed messages

    # Consumer group prefix for all consumers in this domain
    # Group names will be: {prefix}-{worker_name}
    consumer_group_prefix: "verisk"

    # Retry configuration for download retry topics
    # Delays in seconds for each retry attempt: 5min, 10min, 20min, 40min
    retry_delays: [300, 600, 1200, 2400]

    # Maximum number of retry attempts before sending to DLQ
    max_retries: 4

    # -------------------------------------------------------------------------
    # Event Ingester Worker
    # Consumes from: verisk.events.raw
    # Produces to: verisk.enrichment.pending
    # Purpose: Parse events, create enrichment tasks for plugin execution
    # -------------------------------------------------------------------------
    event_ingester:
      processing:
        # Application-level batch size for event processing
        batch_size: 100

        # Optional limit on number of batches to process (for testing)
        # null = unlimited, integer = stop after N batches
        max_batches: null

    # -------------------------------------------------------------------------
    # Download Worker
    # Consumes from: verisk.downloads.pending + verisk.retry
    # Produces to: verisk.downloads.cached
    # Purpose: Download attachments from S3, cache locally for upload
    # -------------------------------------------------------------------------
    download_worker:
      processing:
        # Maximum number of concurrent downloads (1-50)
        # Higher = more throughput but more memory and connections
        concurrency: 10

        # Application batch size for download processing
        batch_size: 20

        # Timeout for individual download operations (seconds)
        timeout_seconds: 60

    # -------------------------------------------------------------------------
    # Upload Worker
    # Consumes from: verisk.downloads.cached
    # Produces to: verisk.downloads.results
    # Purpose: Upload cached files to OneLake, produce results
    # -------------------------------------------------------------------------
    upload_worker:
      processing:
        # Maximum number of concurrent uploads (1-50)
        concurrency: 10

        # Application batch size for upload processing
        batch_size: 20

    # -------------------------------------------------------------------------
    # Delta Events Writer Worker
    # Consumes from: verisk.downloads.results
    # Produces to: delta-events.retry.*, delta-events.dlq
    # Purpose: Write events to Delta tables in batches
    # -------------------------------------------------------------------------
    delta_events_writer:
      processing:
        # Number of events per Delta batch write
        batch_size: 1000

        # Optional limit for testing (null = unlimited)
        max_batches: null

        # Retry delays for failed Delta writes (seconds)
        retry_delays: [300, 600, 1200, 2400]  # 5m, 10m, 20m, 40m

        # Maximum retries before DLQ
        max_retries: 4

        # EventHub entity prefix for retry
        retry_topic_prefix: "com.allstate.pcesdopodappv1.delta-events.retry"

        # Dead letter queue entity
        dlq_topic: "com.allstate.pcesdopodappv1.delta-events.dlq"

  # ===========================================================================
  # CLAIMX DOMAIN CONFIGURATION
  # Verisk ClaimXperience pipeline - claim enrichment and media
  # ===========================================================================
  claimx:
    # -------------------------------------------------------------------------
    # EventHub Entity Names
    # -------------------------------------------------------------------------
    topics:
      events: "com.allstate.pcesdopodappv1.claimx.events.raw"                      # Raw events from Eventhouse
      enrichment_pending: "com.allstate.pcesdopodappv1.claimx.enrichment.pending"  # Events awaiting API enrichment
      enrichment.dlq: "com.allstate.pcesdopodappv1.claimx.enrichment.dlq"          # Enrichment dead letter queue
      enriched: "com.allstate.pcesdopodappv1.claimx.entities.rows"                 # Enriched entity data rows
      downloads_pending: "com.allstate.pcesdopodappv1.claimx.downloads.pending"    # Download tasks awaiting processing
      downloads_cached: "com.allstate.pcesdopodappv1.claimx.downloads.cached"      # Downloads cached locally
      downloads_results: "com.allstate.pcesdopodappv1.claimx.downloads.results"    # Upload results
      dlq: "com.allstate.pcesdopodappv1.claimx.downloads.dlq"                      # Dead letter queue

    consumer_group_prefix: "claimx"

    retry_delays: [300, 600, 1200, 2400]
    max_retries: 4

    # -------------------------------------------------------------------------
    # Event Ingester Worker
    # Consumes from: claimx.events.raw
    # Produces to: claimx.enrichment.pending
    # Purpose: Parse events, route to enrichment
    # -------------------------------------------------------------------------
    event_ingester:
      processing:
        batch_size: 100

    # -------------------------------------------------------------------------
    # Enrichment Worker
    # Consumes from: claimx.enrichment.pending
    # Produces to: claimx.downloads.pending
    # Purpose: Enrich events with ClaimX API data
    # -------------------------------------------------------------------------
    enrichment_worker:
      processing:
        # Maximum concurrent API requests
        api_concurrency: 20

        # Timeout per API request (seconds)
        api_timeout_seconds: 30

        # Batch size for processing
        batch_size: 50

    # -------------------------------------------------------------------------
    # Download Worker
    # Consumes from: claimx.downloads.pending + claimx.retry
    # Produces to: claimx.downloads.cached
    # Purpose: Download attachments from ClaimX S3
    # -------------------------------------------------------------------------
    download_worker:
      processing:
        # ClaimX may need higher concurrency
        concurrency: 15
        batch_size: 30

        # ClaimX files may be larger, need longer timeout
        timeout_seconds: 90

    # -------------------------------------------------------------------------
    # Upload Worker
    # Consumes from: claimx.downloads.cached
    # Produces to: claimx.downloads.results
    # Purpose: Upload cached files to OneLake
    # -------------------------------------------------------------------------
    upload_worker:
      processing:
        concurrency: 15
        batch_size: 30

    # -------------------------------------------------------------------------
    # Delta Events Writer Worker
    # Consumes from: claimx.downloads.results
    # Produces to: delta-events.retry.*, delta-events.dlq
    # Purpose: Write events to Delta tables
    # -------------------------------------------------------------------------
    delta_events_writer:
      processing:
        batch_size: 1000
        retry_delays: [300, 600, 1200, 2400]
        max_retries: 4

  # ===========================================================================
  # SHARED STORAGE CONFIGURATION
  # ===========================================================================
  storage:
    # OneLake paths by domain (abfss:// URIs)
    # Format: abfss://workspace@onelake.dfs.fabric.microsoft.com/lakehouse/Files/{domain}
    onelake_domain_paths:
      verisk: ""   # OneLake path for Verisk domain
      claimx: ""   # OneLake path for ClaimX domain

    # Fallback path if domain not in onelake_domain_paths
    onelake_base_path: ""

    # Local directory for caching downloads awaiting upload
    cache_dir: "/tmp/pipeline_cache"


# =============================================================================
# DELTA LAKE TABLE CONFIGURATION
# =============================================================================
# Paths to Delta tables for the pipeline. These support both verisk and claimx domains.
# Format: abfss://workspace@onelake.dfs.fabric.microsoft.com/lakehouse/Tables/{table}
delta:
  # Enable/disable Delta Lake writes globally
  enable_writes: true

  # Verisk domain tables
  verisk:
    # Path to verisk_events table (for deduplication and analytics)
    events_table_path: ""

    # Path to verisk_attachments table (for tracking downloaded attachments)
    inventory_table_path: ""

    # Path to verisk_attachments_failed table (for tracking permanent failures)
    # Optional: If not set, permanent failures are only tracked in DLQ
    failed_table_path: ""

  # ClaimX domain tables
  claimx:
    # Path to claimx_events table
    events_table_path: ""
    projects_table_path: ""
    contacts_table_path: ""
    media_table_path: ""
    tasks_table_path: ""
    task_templates_table_path: ""
    external_links_table_path: ""
    video_collab_table_path: ""


# =============================================================================
# CLAIMX API CONFIGURATION
# =============================================================================
claimx:
  api:
    # Base URL for ClaimX API
    base_url: "https://api.claimx.com/v1"

    # Timeout for API requests (seconds)
    timeout_seconds: 30

    # Maximum concurrent API requests
    max_concurrent: 20

    # Credentials are loaded from environment variables ONLY:
    # - CLAIMX_API_TOKEN (Base64 encoded Basic auth token)


# =============================================================================
# EVENTHOUSE INTEGRATION (for pipeline event source)
# =============================================================================
eventhouse:
  # ---------------------------------------------------------------------------
  # Connection Settings
  # ---------------------------------------------------------------------------
  # Eventhouse cluster URL
  cluster_url: ""

  # Database name
  database: ""

  # ---------------------------------------------------------------------------
  # Query Settings
  # ---------------------------------------------------------------------------
  # Maximum time for query execution (seconds)
  query_timeout_seconds: 120

  # Maximum retry attempts for failed queries
  max_retries: 3

  # Base delay for exponential backoff (seconds)
  retry_base_delay_seconds: 1.0

  # Maximum delay between retries (seconds)
  retry_max_delay_seconds: 30.0

  # Maximum concurrent connections
  max_connections: 10

  # ---------------------------------------------------------------------------
  # Poller Settings
  # ---------------------------------------------------------------------------
  poller:
    # How often to poll for new events (seconds)
    poll_interval_seconds: 30

    # Number of events to fetch per poll
    batch_size: 1000

    # Source table to query for events
    source_table: "Events"

    # Path to events table for tracking processed events
    events_table_path: ""

    # Backpressure: pause polling if EventHub lag exceeds this
    max_eventhub_lag: 10000

    # How often to check EventHub lag (seconds)
    lag_check_interval_seconds: 60

    # Column mapping from Eventhouse to EventMessage fields
    column_mapping:
      trace_id: "trace_id"
      event_type: "event_type"
      event_subtype: "event_subtype"
      timestamp: "timestamp"
      source_system: "source_system"
      payload: "payload"
      attachments: "attachments"

  # ---------------------------------------------------------------------------
  # Deduplication Settings
  # ---------------------------------------------------------------------------
  dedup:
    # Path to verisk events table for deduplication
    events_table_path: ""

    # Look back this many hours in verisk events for dedup (hours)
    verisk_events_window_hours: 24

    # Look back this many hours in Eventhouse for new events (hours)
    eventhouse_query_window_hours: 1

    # Overlap between queries to avoid missing events (minutes)
    overlap_minutes: 5

    # Maximum trace IDs to check per dedup query
    max_trace_ids_per_query: 50000


# =============================================================================
# DUMMY DATA SOURCE (for testing without external dependencies)
# =============================================================================
# When event_source is set to "dummy", the pipeline generates realistic
# synthetic insurance claim data instead of connecting to external sources.
# This allows testing the full pipeline locally without access to production
# systems like Eventhouse, Event Hub, or the ClaimX API.
dummy:
  # ---------------------------------------------------------------------------
  # Generator Settings
  # ---------------------------------------------------------------------------
  generator:
    # Random seed for reproducible data (null = random each run)
    seed: null

    # Base URL for the dummy file server (auto-configured when running)
    base_url: "http://localhost:8765"

    # Include some failing scenarios for testing error handling
    include_failures: false

    # Percentage of events that will have simulated failures (0.0-1.0)
    failure_rate: 0.05

  # ---------------------------------------------------------------------------
  # File Server Settings
  # ---------------------------------------------------------------------------
  # The dummy file server provides downloadable test files for attachments
  file_server:
    host: "0.0.0.0"
    port: 8765

    # Default size for generated files (bytes)
    default_file_size: 100000  # 100KB

    # Maximum file size to generate (bytes)
    max_file_size: 10000000  # 10MB

  # ---------------------------------------------------------------------------
  # Event Generation Settings
  # ---------------------------------------------------------------------------
  # Which domains to generate events for
  domains:
    - "verisk"
    - "claimx"

  # Events generated per minute (per domain)
  events_per_minute: 10.0

  # Burst mode: generate events in batches for load testing
  burst_mode: false
  burst_size: 50
  burst_interval_seconds: 60

  # Generate all ClaimX event types (PROJECT_CREATED, PROJECT_FILE_ADDED, etc.)
  include_all_event_types: true

  # Maximum number of concurrent simulated claims to track
  max_active_claims: 100

  # ---------------------------------------------------------------------------
  # Runtime Limits (for controlled testing)
  # ---------------------------------------------------------------------------
  # Stop after generating this many events (null = unlimited)
  max_events: null

  # Stop after running for this many seconds (null = unlimited)
  max_runtime_seconds: null


# =============================================================================
# SHARED SETTINGS
# =============================================================================
health:
  enabled: true
  host: "127.0.0.1"
  port: 8080

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_dir: "logs"

observability:
  json_logs: true
  log_upload_enabled: true
  log_upload_interval_cycles: 10
  log_retention_days: 7
  memory_checkpoints_enabled: true
  memory_checkpoint_level: "DEBUG"
  memory_profiling_enabled: true
  memory_snapshot_interval: 5
  memory_alert_threshold_mb: 4096

# Runtime metadata
worker_id: "worker-01"
test_mode: false
